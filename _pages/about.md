---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
# üëã Biography [[Google Scholar]](https://scholar.google.com/citations?user=V3XULuMAAAAJ)

Hanbo Bi is a 4th-year PhD candidate at the 
[Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS)](http://www.aircas.ac.cn/) 
and [University of Chinese Academy of Sciences (UCAS)](https://www.ucas.ac.cn/),
supervised by Prof. Hongqi Wang and 
[Prof. Xian Sun](https://people.ucas.ac.cn/~sunxian) (NSFC Distinguished Young Scholar). He has published multiple first-author papers in top international journals, including IEEE TPAMI, IJCV, and IEEE TGRS, and serves as a reviewer for prestigious publications such as IJCV, IEEE TIP, and IEEE TCSVT.

His research focuses on computer vision, encompassing **vision foundation models**, **self-supervised learning**, **few-shot learning**, and **remote sensing image interpretation**. Notably, his leading work on **RingMoE**, a 10-billion-parameter remote sensing vision foundation model developed at AIRCAS, was reported by CCTV and selected as one of the annual significant academic advances in remote sensing imagery at VALSE 2025. Currently, he is shifting his research focus to **multi-modal vision-language models**.


# üî• News
- *2025.11*: One papers about **Multimodal Language Model for Dialogue Understanding** is accepted by ‚Äã**AAAI**, CCF-A.
- *2025.07*: One papers about **Segment Anything Model** have been submitted to ‚Äã**arXiv**.
- *2025.06*: One paper is accepted by **ICCV Oral**, CCF-A.
- *2025.04*: One paper is accepted by **IEEE JSTAR**, Top, IF 5.3.
- *2025.04*: Two papers about **Vision Foundation Models** have been submitted to ‚Äã**arXiv**.
- *2025.03*: One paper is accepted by **ISPRS**, Q1, Top, IF 12.2.
- *2025.02*: One paper is accepted by **IEEE JSTAR**, Top, IF 5.3.
- *2024.12*: **National Scholarship** for Doctoral Students.
- *2024.11*: One paper about **Vision Foundation Models** has been submitted to ‚Äã**arXiv**.
- *2024.10*: One paper is accepted by **IJCV**, CCF-A, Top, IF 11.6.
- *2024.10*: **National Key Laboratory Scientific Research Innovation Fund Project**, Leader.
- *2024.09*: One paper about **Vision Foundation Models** has been submitted to **‚ÄãarXiv**.
- *2024.09*: One paper is accepted by **IEEE TPAMI**, CCF-A, Top, IF 23.6.
- *2024.09*: One paper is accepted by **IEEE TGRS**, CCF-B, Top, IF 8.6.
- *2024.08*: One paper is accepted by **IEEE TGRS**, CCF-B, Top, IF 8.6.
- *2024.05*: One paper is accepted by **IEEE TGRS**, CCF-B, Top, IF 8.6.
- *2024.04*: One paper is accepted by **IEEE TGRS**, CCF-B, Top, IF 8.6.
- *2023.10*: One paper is accepted by **IEEE TGRS**, CCF-B, Top, IF 8.6.
- *2023.05*: One paper is accepted by **National Science Review (NSR)**, Top, IF 17.1.


# üìù Publications 

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/PAT.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot Segmentation** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/106812536)

  **Hanbo Bi**, Yingchao Feng, Wenhui Diao, Peijin Wang, Yong-Qiang Mao, Kun Fu, Hongqi Wang, Xian Sun
  
  *IEEE Transactions on Pattern Analysis and Machine Intelligence* ***(IEEE TPAMI 2024, CCF-A, Top, IF=23.6)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/AgMTR.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **AgMTR: Agent Mining Transformer for Few-shot Segmentation in Remote Sensing** [[**Paper Link**]](https://link.springer.com/article/10.1007/s11263-024-02252-y)

  **Hanbo Bi**, Yingchao Feng, Yong-Qiang Mao, Jianning Pei, Wenhui Diao, Hongqi Wang, Xian Sun
  
  *International Journal of Computer Vision* ***(IJCV 2024, CCF-A, Top, IF=11.6)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/DMNet.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Not just learning from others but relying on yourself: A new perspective on few-shot segmentation in remote sensing** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10288551)

  **Hanbo Bi**, Yingchao Feng, Zhiyuan Yan, Yong-Qiang Mao, Wenhui Diao, Hongqi Wang, Xian Sun
  
  *IEEE Transactions on Geoscience and Remote Sensing* ***(IEEE TGRS 2023, CCF-B, Q1, Top, IF=8.6)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/RingMoE.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation** [[**Paper Link**]](https://arxiv.org/abs/2504.03166)

  **Hanbo Bi**, Yingchao Feng, Boyuan Tong, Mengyu Wang, Haichen Yu, Yong-Qiang Mao, Hao Chang, Wenhui Diao, Peijin Wang, Yue Yu, Hanyang Peng, Yehong Zhang, Kun Fu, Xian Sun
  
  *arXiv* ***(One of the annual significant academic advances in remote sensing imagery at VALSE 2025, under review)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/f2rvlm.jpg' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **F¬≤RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model** [[**Paper Link**]](https://f2rvlm.github.io/)

  **Hanbo Bi**
  *AAAI Conference on Artificial Intelligence* ***(AAAI 2026)***  

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/SAM.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation** [[**Paper Link**]](https://arxiv.org/abs/2507.02294)

  **Hanbo Bi**, Yulong Xu, Ya Li, Yong-Qiang Mao, Boyuan Tong, Chongyang Li, Chunbo Lang, Wenhui Diao, Hongqi Wang, Yingchao Feng, Xian Sun
  
  *arXiv* ***(under review)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/TIP.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning** [[**Paper Link**]](https://arxiv.org/abs/2504.11999)

  Mengyu Wang\*, **Hanbo Bi\***, Yingchao Feng, Linlin Xin, Shuo Gong, Tianqi Wang, Zhiyuan Yan, Peijin Wang, Wenhui Diao, Xian Sun
  
  *arXiv* ***(\*: equal contribution, under review)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/VHeat.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model** [[**Paper Link**]](https://arxiv.org/abs/2411.17984)

  Huiyang Hu, Peijin Wang, **Hanbo Bi**, Boyuan Tong, Zhaozhi Wang, Wenhui Diao, Hao Chang, Yingchao Feng, Ziqi Zhang, Yaowei Wang, Qixiang Ye, Kun Fu, Xian Sun
  
  *International Conference on Computer Vision* ***(ICCV 2025, Oral)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/ISPRS.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Twin deformable point convolutions for airborne laser scanning point cloud classification** [[**Paper Link**]](https://www.sciencedirect.com/science/article/abs/pii/S092427162500036X)

  Yong-Qiang Mao, **Hanbo Bi**, Xuexue Li, Kaiqiang Chen, Zhirui Wang, Xian Sun, Kun Fu
  
  *ISPRS Journal of Photogrammetry and Remote Sensing* ***(ISPRS 2025, Q1, Top, IF=12.2)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/Twin.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **SDL-MVS: View Space and Depth Deformable Learning Paradigm for Multi-View Stereo Reconstruction in Remote Sensing** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10684788)

  Yong-Qiang Mao, **Hanbo Bi**, Liangyu Xu, Kaiqiang Chen, Zhirui Wang, Xian Sun, Kun Fu
  
  *IEEE Transactions on Geoscience and Remote Sensing* ***(IEEE TGRS 2024, CCF-B, Q1, Top, IF=8.6)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/Attn.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Attention-based contrastive learning for few-shot remote sensing image classification** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10493055)

  Yulong Xu, **Hanbo Bi**, Hongfeng Yu, Wanxuan Lu, Peifeng Li, Xinming Li, Xian Sun
  
  *IEEE Transactions on Geoscience and Remote Sensing* ***(IEEE TGRS 2024, CCF-B, Q1, Top, IF=8.6)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/ILI.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Injecting linguistic into visual backbone: Query-aware multimodal fusion network for remote sensing visual grounding** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10648751)

  Chongyang Li, Wenkai Zhang, **Hanbo Bi**, Jihao Li, Shuoke Li, Haichen Yu, Xian Sun, Hongqi Wang
  
  *IEEE Transactions on Geoscience and Remote Sensing* ***(IEEE TGRS 2024, CCF-B, Q1, Top, IF=8.6)*** 

  </div>
</div>


<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/PETL.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Efficient Side-Tuning for Remote Sensing: A Low-Memory Fine-Tuning Framework** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10974700)

  Haichen Yu, Wenxin Yin, **Hanbo Bi**, Chongyang Li, Yingchao Feng, Wenhui Diao, Xian Sun
  
  *IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing* ***(IEEE JSTAR 2025, Top, IF=5.3)*** 

  </div>
</div>


<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/Retrieval.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **FRORS: An Effective Fine-Grained Retrieval Framework for Optical Remote Sensing Images** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10904305)

  Yong-Qiang Mao, Zhizhuo Jiang, Yu Liu, Yiming Zhang, Kehan Qi, **Hanbo Bi**, You He
  
  *IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing* ***(IEEE JSTAR 2025, Top, IF=5.3)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/TAFormer.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Taformer: A unified target-aware transformer for video and motion joint prediction in aerial scenes** [[**Paper Link**]](https://ieeexplore.ieee.org/abstract/document/10522786)

  Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Yong-Qiang Mao, **Hanbo Bi**, Chenglong Liu, Xian Sun, Kun Fu
  
  *IEEE Transactions on Geoscience and Remote Sensing* ***(IEEE TGRS 2024, CCF-B, Q1, Top, IF=8.6)*** 

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/NSR.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **Automated object recognition in high-resolution optical remote sensing imagery** [[**Paper Link**]](https://academic.oup.com/nsr/article/10/6/nwad122/7152628?login=false)


  Yazhou Yao, Tao Chen, **Hanbo Bi**, Xinhao Cai, Gensheng Pei, Guoye Yang, Zhiyuan Yan, Xian Sun, Xing Xu, Hai Zhang
  
  *National Science Review* ***(NSR 2023, Top, IF=17.1)*** 
  </div>
</div>


# üöÄ Projects

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/ringmo3.0.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **RingMo 3.0: Mixture-of-Modality-Experts Vision Foundation Models for Universal Remote Sensing Image Interpretation with 10-Billion-Parameters** [[**Project Link**]](https://arxiv.org/abs/2504.03166)
  - *2024.03 - 2024.12*, Student Leader
  - Collaboration with Academician Wen Gao's Team at Pengcheng Laboratory
  - Construct the world's first 10-billion-parameter unified multi-modal foundation model for remote sensing, leveraging massive multi-modal heterogeneous remote sensing data
  - Achieved **SOTA performance** on **23 international benchmarks** across classification, detection, segmentation, tracking, change detection, and depth estimation
  - The sole specialized remote sensing foundation model of the **2030 New Generation Artificial Intelligence National Key R&D Program flagship project** (ÁßëÊäÄÂàõÊñ∞2030ÈáçÂ§ß‰∏ìÈ°π‚ÄúÊñ∞‰∏Ä‰ª£‰∫∫Â∑•Êô∫ËÉΩ‚ÄùÂîØ‰∏Ä‰∏ì‰∏öÈÅ•ÊÑüÂü∫Á°ÄÊ®°Âûã)
  - Reported in **CCTV News coverage** and selected as one of the **annual significant academic advances in remote sensing imagery at VALSE 2025**

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <img src='images/ringmo2.png' alt="ËÆ∫ÊñáÁ§∫ÊÑèÂõæ" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">
  
  **RingMo 2.0: Multi-modal Remote Sensing Foundation Model with 1-Billion-Parameters** [[**Project Link**]](https://gitee.com/mindspore/ringmo-framework)
  - *2023.03 - 2023.08*, Student Leader
  - Constructed a 1-billion-parameter multi-modal remote sensing foundation model via self-supervised pre-training, leveraging 16 million optical/SAR remote sensing images
  - Successfully **deployed in industrial scenarios** (including Industrial and Commercial Bank of China (ICBC) and aerospace research institutes) and obtained application certificates from partner organizations
  - Successfully adapted the model to Pytorch/MindSpore framework
  </div>
</div>


# ‚úçÔ∏è Reviewer Services
- IEEE Transactions on Image Processing (IEEE TIP)
- International Journal of Computer Vision (IJCV)
- IEEE Transactions on Multimedia (IEEE TMM)
- IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)
- IEEE Transactions on Geoscience and Remote Sensing (IEEE TGRS)
- IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (IEEE JSTARS)
- Remote Sensing
- Scientific Reports


# üíª Internships
- *2025.03 - Present*, **Tencent WeiXin Group (WXG)**, Technology Architecture Department, Beijing, China

# üìñ Educations
- *2021.09 - Present*, **Doctor of Engineering**, Institute of Aerospace Information Innovation, Chinese Academy of Sciences, Beijing, China
- *2017.09 - 2021.06*, **Bachelor of Engineering**, School of Electrical and Information Engineering, Hunan University, Changsha, China

# üéñ Honors and Awards
- *2024.12* National Scholarship for Doctoral Students (0.3%)
- *2024.10* National Key Laboratory Scientific Research Innovation Fund Project (Leader)
- *2023.06* Outstanding Party Member of the University of Chinese Academy of Sciences
- *2022.06* Outstanding Student and Outstanding Communist Youth League Cadre of the University of Chinese Academy of Sciences
- *2021.06* Outstanding Graduate of Hunan Province and Outstanding Undergraduate Theses at Hunan University
- *2020.12* National Scholarship for Undergraduate Students (1/94)
- *2020.05* Huawei Scholarship (1/94)
- *2019.12* National Scholarship for Undergraduate Students (1/94)

<div style="text-align: center; margin: 30px 0; padding: 20px; border-top: 1px solid #eee;">
  <a href="https://clustrmaps.com/site/1c6rd" title="ClustrMaps">
    <img 
      src="//www.clustrmaps.com/map_v2.png?d=A6rj7tdSBMdxv2GI6adIAqpa1CwnJDvaHZSWz7oiYXk&cl=ffffff" 
      alt="ËÆøÂÆ¢ÁªüËÆ°Âú∞Âõæ" 
      style="max-width: 100%; height: auto; border: none; display: block; margin: 0 auto;"
    />
  </a>
</div>


